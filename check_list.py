# Чек-лист на основе статьи 50 оттенков Celery
# https://habr.com/ru/company/oleg-bunin/blog/433476/

# ==============================================================================================
# 1. Оформление кода
# ==============================================================================================
'''
Минимум логики в задаче. 
Используем задачки только как «запускаторы» кода. 
То есть задача не несет в себе логику, а триггерит запуск кода в Background.
'''
# ---------------------------------------------
@celery_app.task(queue='...')
def run_regular_update(provider_account_id, *args, **kwargs):
    """..."""
    flow = flows.RegularSyncProviderAccountFlow(provider_account_id) 
    return flow.run(*args, **kwargs)
# ---------------------------------------------

# ==============================================================================================
# 2. Простые объекты в параметрах
# ==============================================================================================
'''
Передаем только маленькие скалярные данные, id, в параметры
Нк храним в памяти очереди большие блоки объектов

Использования одного рабочего с соответствующим количеством параллельных процессов для уменьшения дублирования.
Поскольку каждая задача распределяется между каждым рабочим, это означает, 
что требования к памяти = размер полезной нагрузки * количество задач в очереди * количество рабочих, 
которые вызывали проблемы с памятью. В этом случае использование 1 рабочего сократит использование памяти. 
Однако лучшим решением было не пропускать такую ​​большую полезную нагрузку.

Т.е. если разные таски должны работать с одноими и теми же входными данными - то эти данные
передаются в каждую дочку, что увеличивает расход памяти.
Передали кусок базы на 10 мб в 3 таска = 30 мб заняли в очереди, потом этот же объем пойдет в дочек.
Почему множнется на кол-во рабочих - надо уточнять...

Разница в сельдерее между параллелизмом, рабочими процессами и автомасштабированием
https://stackoverflow.com/questions/31898311/celery-difference-between-concurrency-workers-and-autoscaling
'''

# ==============================================================================================
# 3. Идемпотентные задачи
# ==============================================================================================
"""
Идемпоте́нтность — свойство объекта или операции 
при повторном применении операции к объекту давать тот же результат, 
что и при первом
Не всегда этого просто добиться, особенно если идет двухфазные коммиты с базой.
Всегда проверяйте, что входящие данные существуют и актуальны,
над ними действительно можно совершить работу, 
и использовать транзакции. 
Если к одной задаче много запросов в базу и что-то может пойти не так во время выполнения
— используйте транзакции, чтобы откатить ненужные изменения.
Хорошая статья про транзакции:
https://habr.com/ru/post/537594/
"""

# ==============================================================================================
# 4. Обратная совместимость между релизами
# ==============================================================================================
"""
Когда старый код сервиса создает сообщения для нового кода воркера,
и наоборот, старый воркер принимает сообщения от нового кода сервиса,
потому что он раскатился «первее» и туда трафик пошел.
Во всех задачах мы сейчас делаем «резиновую» сигнатуру (**kwargs).
Когда в следующем релизе вам потребуется добавить новый параметр,
вы его из **kwargs возьмете в новом релизе,
а в старом не возьмете — у вас ничего не сломается
Иначе когда меняется сигнатура, а Celery об этом не знает
- он падает и выдает ошибку, что такого параметра нет в задаче.
"""

# ==============================================================================================
# 5. Таймауты
# ==============================================================================================
"""
Не ставить таймаут на задачу — это зло.
У нас обвешаны таймаутами, в том числе глобальными для всех задач, 
и для каждой конкретной задачи тоже проставлены таймауты.
"""


"""Лимит на выполнение задачи"""

'''Глобально (можно переопределять локально)'''
# CELERY_TASK_TIME_LIMIT - через сколько секунд уже выполняемая задача будет убита, совсем.
# Должна быть дольше чем soft limit
# По умолчанию None
task_time_limit = 3600

# CELERY_TASK_SOFT_TIME_LIMIT - черезе сколько секунд выйдет исключение SoftTimeLimitExceeded
# по уже выполняемой задаче
# после чего поведение можно переопределить или Очистить задачу, пока задача не убита.
# Либо запустить заново: https://stackoverflow.com/questions/27407888/how-to-retry-celery-task-on-hard-timeout
# По умолчанию None
task_soft_time_limit = 3540

# ---------------------------------------------
from celery.exceptions import SoftTimeLimitExceeded

@app.task
def mytask():
    try:
        return do_work()
    except SoftTimeLimitExceeded:
        cleanup_in_a_hurry() # Очистить задачу
# ---------------------------------------------

'''Через таск (можно переопределять при вызове задачи)'''
# soft_limit_timeout и time_limit - переопределяют глобальные (task_soft_time_limit и task_time_limit)
# ---------------------------------------------
@app.task(soft_time_limit=60, time_limit=70)
def mytask():
    ...
# ---------------------------------------------

"""countdown и expires """
# через сколько приступить, и когда уже не актуально, просрочено
# Когда работник получает просроченную задачу, он помечает задачу как Отозваная и не исполняет ее

"""запуск через таск"""
# ---------------------------------------------
# выполняется через одну минуту, но истекает через 1 день 
@app.task(countdown=60, expires=datetime.now() + timedelta(days=1))
def mytask(x,y):
    ...
# ---------------------------------------------

"""запуск или переопределение чере вызов"""
mytask.apply_async((10, 10), countdown=10, expires=120)

# ==============================================================================================
# 6. Retry Policy
# ==============================================================================================
"""
Политика повторных попыток запуска тасков 

Для тех задач, которые можно повторить, или которые могут выполниться с ошибками, 
мы используем Retry policy. Но используем аккуратно, чтобы не завалить внешние сервисы. 
Если быстро повторять задачи, не указывая exponential backoff (retry_backoff), то внешний сервис, 
а может быть и внутренний, могут просто не выдержать
"""

"""При сбоях СОЕДИНЕНИЯ"""

"""Глобально"""
# CELERY_PUBLISH_RETRY
# - Определяет, будет ли повторяться публикация сообщений о задачах 
# в случае потери соединения или других ошибок СОЕДИНЕНИЯ
# По умолчанию True 
task_publish_retry = True 


# CELERY_PUBLISH_RETRY_POLICY
# - Определяет политику по умолчанию при повторной публикации сообщения
# в случае потери соединения или других ошибок СОЕДИНЕНИЯ
# По умолчанию как в примере ниже
task_publish_retry_policy = {
     "max_retries" : 3 ,        # Максимальное кол-во повторных попыток перед отказом, затем исключение
     "interval_start" : 0 ,     # Кол-во количество секунд (int/float) ожидания между повторными попытками
     "interval_step" : 0,2 ,    # секунд (int/float) При каждой последующей поптке это число будет добавляться к задержке повторной попытки
     "interval_max" : 0,2       # секунд (int/float) Максимальное кол-во секунд для ожидания между повторными попытками. 
}

"""Задание или переопределение локльно через вызов"""
# ---------------------------------------------
mytask.apply_async((2, 2), retry=True, retry_policy={
    'max_retries': 3,
    'interval_start': 0,
    'interval_step': 0.2,
    'interval_max': 0.2,
})
# ---------------------------------------------

"""При известных исключениях"""

"""Только локально через таск"""
# ---------------------------------------------
# Автоматическое повторение задачи, без указания raise self.retry(exc=exc)
@app.task(
  autoretry_for=(EmailServerNotReachable, SomeEmailException, ), # Список / кортеж классов исключений, по умолчанию пусто
  max_retries = 5,          # Максимальное количество попыток перед отказом, по умолчанию 3
  retry_backoff = True,     # По умолчанию False - нет задержек между попытками; bool или int:
                            # True - автоматические повторные попытки будут отложены на 1,2,4,8... сек
# retry_backoff=3,          # 3 - Повторная попытка через 3,6,12,24... секунды
  retry_backoff_max = 700,  # Масимальная задержка для retry_backoff. По умолчанию 600 сек = 10 минутам
  retry_jitter=True,        # фактическая задержка будет от 0 до retry_backoff вычисленное на данном повторе, умолчание=True
)
def mytask():
  ...
# ---------------------------------------------
# Повторение задачи, с указаним raise self.retry(exc=exc)
@app.task(bind=True, max_retries = 5, soft_time_limit=100, default_retry_delay=30)
def send_twitter_status(self, oauth, tweet):
    try:
        twitter = Twitter(oauth)
        twitter.update_status(tweet)
    except (Twitter.FailWhaleError, Twitter.LoginError) as exc:
        raise self.retry(exc=exc, countdown=60) # Не создаст выборос, а пойдет на повтор, иначе выдаст исключение
# Обзаятально указываем exc=exc
# default_retry_delay=30 - указали, что повтораня попытка через 30 сек
# countdown=60 - переопределили, что повторная поатка через 1 минуту

# ---------------------------------------------

# ==============================================================================================
# 7. Утечки памяти
# ==============================================================================================
"""
К сожалению, утечки памяти возникают очень легко, а найти и исправить их сложно.
В целом работа с памятью у Python очень спорная. 
Вы потратите много времени и нервов, чтобы понять, почему происходит утечка, а потом выяснится, 
что она даже не в вашем коде. 
Поэтому всегда, начиная проект, 
проставляйте лимит памяти на воркер: worker_max_memory_per_child. (не на воркер а, на дочерний процесс!)
Это гарантирует, что однажды не придет OOM Killer, не убьет все воркеры (процессы!), 
и вы не потеряете все задачи. Celery (воркер!) будет сам перезапускать воркеры (процессы!), когда нужно.
"""

"""Глобально"""

"""Ограничение памяти на дочерний процесс"""
#  - маскимальное кол-во памяти которое выделяется на один дочерний процесс
# в случае превышения, текущая задача доч. процессом будет выполнена, и он будет перезагружен.
# последующие задачи пойдут на другой дочерний процесс.
# По умолчанию - не установлено, безлимит
worker_max_memory_per_child = 12000 # 12MB


"""Ограничение задач на дочерний процесс"""
# Максимальное количество задач, которые может выполнить рабочий процесс пула,
# прежде чем он будет заменен новым. 
# По умолчанию ограничений нет.
worker_max_tasks_per_child = 5


"""Контроль предварительной выборки"""

"""
Сделано для того чтобы по 100 раз не бегать в базу за задачами, экономия вермени.
Если мы установим слишком большую выборку, то это увеличит потребляемую память на процесс, 
а если при этом мы сильно уменьшим ограничение по пямити на процесс,
то каждый рабочий будет выполнять 1 задачу и погибать после каждой задачи.
Что будет влиять на время, пока он перезапускается.
https://github.com/celery/celery/issues/4125
"""
# CELERYD_PREFETCH_MULTIPLIER
#  - маскимальное кол-во тасков которое хватает воркер на один процесс
# Т.е. если значение 3, а ядер 8, то будет зарезервировано 24 таска
# Задачи будут раздаваться по 3 на каждое ядро (если указан лимит 3), по очереди.
# https://docs.celeryproject.org/en/stable/userguide/optimizing.html#prefetch-limits
# https://stackoverflow.com/questions/66526540/is-it-possible-to-set-worker-prefetch-multiplier-for-each-queue-separately-in
# https://stackoverflow.com/questions/16040039/understanding-celery-task-prefetching
# По умолчанию 4 таска на ядро
worker_prefetch_multiplier = 3

# ==============================================================================================
# 8. Приоритет выполнения задач
# ==============================================================================================
"""
Мы используем другой подход — отдельные воркеры для приоритетов, 
т.е. по старинке создаем воркеры для Celery с разными «важностями».

Авторы запускают Celery multi start, т.е. используют демонизацию, запуск в фоновом режиме,
А для этого надо сделать предварительные настройки в systemd, с правами суперпользователя.
https://docs.celeryproject.org/en/stable/userguide/daemonizing.html#daemonizing

Иной путь - запуск каждого воркера из разной консоли, что нас устраивает, т.к.
будем использовать Docker Compose, разные контейнеры.
"""
# ---------------------------------------------
# celery multi start 
# high_priority low_priority 
# -c:high_priority 2 -c:low_priority 6 
# -Q:high_priority urgent_notifications 
# -Q:low_priority emails,urgent_notifications 

celery -A celery_worker:app worker -n high_priority -Q urgent_notifications -c 2
celery -A celery_worker:app worker -n low_priority -Q emails,urgent_notifications -c 6
# ---------------------------------------------

"""
Два воркера (доч. процесса) high_priority постоянно обрабатывают очередь urgent_notifications. 
Эти воркеры (доч. процесса) больше никто не займет, 
они будут только читать важные задачи из очереди urgent_notifications.

Для неважных задач есть low_priority очередь. 
Там 6 воркеров (доч. процесса), которые принимают сообщения из всех остальных очередей. 
Также low_priority воркеры (доч. процессы) мы подписываем на urgent_notifications, 
чтобы они могли помочь, если воркеры (доч. процессы) с high_priority не будут справляться.
"""

"""Как направлять таски в нужную очередь"""

"""Глобально"""
# CELERY_ROUTES - маршрутизация тасков по очередям
# По умолчанию: None.
# ---------------------------------------------
"""
В конфигурации указываем, что задачи, 
чьи имена начинаются с "uuu.*" направляем в очередь "urgent_notifications"
для этого прямо задаем имя каждому таску
...
"""
"task_routes": {
    'uuu.*': {'queue': 'urgent_notifications'},
    'wweb.*': {'queue': 'emails'},
}

@app.task(name='uuu.PRverka')
def mytask():
    print("ПРоверка связи 2")
    return "OK"
# ---------------------------------------------

"""Задание или переопределение локльно через таск"""
@app.task(name='Vobla', queue='queue1')
def app1_test():
    ...

# ==============================================================================================
# 9. Extract, Transform, Load - ETL
# ==============================================================================================
"""
Извлечь, Преобразовать, Загрузить
Задачи, каждая из которых получает на вход данные из предыдущей задачи.
"""
# ---------------------------------------------
@task
def download_account_data(account_id)
    …
    return account_id

@task
def process_account_data(account_id, processing_type)
    …
    return account_data

@task
def store_account_data(account_data)
# ---------------------------------------------

"""
Решение:
distributed processing - распределенная обработка
https://docs.celeryproject.org/en/stable/reference/celery.html
Для более сложных цепочек приходится подключать дополнительную логику. 
Но важно иметь в виду, что если в этом chain возникнет проблема в одной задаче, то весь chain развалится. 
Если вы не хотите такого поведения, то обрабатывайте exception и продолжайте выполнение, 
либо останавливайте всю цепочку по исключению.

Остановка chain:
https://stackoverflow.com/questions/17461374/celery-stop-execution-of-a-chain

"""
# ---------------------------------------------
"""Локально через вызов,
при этом каждый таск может иметь свои настройки таймаута и перерзапуска"""
chain(
    download_account_data.s(account_id), 
    process_account_data.s(processing_type='fast'), 
    store_account_data.s()
).delay()

# Пример: ((2 + 2) + 4):
>>> res = chain(add.s(2, 2), add.s(4))()
>>> res.get()
8
# ---------------------------------------------
"""
Celery сам разберет pipeline, выполнит по порядку сначала первую задачу, 
потом полученные данные передаст во вторую, данные, которые вернет вторая задача, передаст в третью. 
Так мы реализуем простые ETL pipelines.
Что такое .s смотри в СПРАВОЧНАЯ ИНФОМАЦИЯ
"""

# ==============================================================================================
# ==============================================================================================



# ххх. Команды в воркере

celery -A celery_worker:app worker  # celery_worker - модуль где находится прилоежние селери
                                    # :app - экземпляр приложения. Можно явно не указывать
                                    # worker - далее параметры worker-а
-n flask_1@%h           # За дали свое имя для воркера, в т.ч. хост
--loglevel=info         # Уровень логирования ошибок, что отлавливать и выдавать
-Q proverky_brosss      # указали имя отслеживаемой очереди
--prefetch-multiplier=3 # Кол-во тасков на ядро + резевирование
-с 2                    # Кол-во дочерних процессов на воркер


# ==============================================================================================
# ==============================================================================================
"""
СПРАВОЧНАЯ ИНФОМАЦИЯ

bind - Привязка задачи означает, что первым аргументом задачи всегда будет экземпляр задачи (self)
https://coderoad.ru/54899320/Что-означает-ключевое-слово-bind-True-в-celery

.s - .signature - "Подпись" используется вместе с цепочкой для создания рабочего процесса. 
«.s» - это сокращение от «.signature». 
при использовании ".s" означает, что результат или возвращаемое значение передней задачи будет передано следующей. 
Противоположностью «подписи» является «неизменная подпись», в которой каждая задача независима.
https://docs.celeryproject.org/en/stable/reference/celery.html#celery.chain
https://docs.celeryproject.org/en/latest/userguide/canvas.html#signatures

@shared_task или @app.task
https://stackoverflow.com/questions/54506515/difference-between-different-ways-to-create-celery-task/54510022
https://stackoverflow.com/questions/21233089/how-to-use-the-shared-task-decorator-for-class-based-tasks

Конфигурация и настройки по умолчанию
https://docs.celeryproject.org/en/stable/userguide/configuration.html#std-setting-task_publish_retry

Конфигурация и настройки по умолчанию (тип данных и значения по умолчанию)
https://github.com/andrewp-as-is/django-configurations-celery.py

Конфигурации pool: solo prefork eventlet gevent
https://medium.com/analytics-vidhya/python-celery-explained-for-beginners-to-professionals-part-3-workers-pool-and-concurrency-ef0522e89ac5

Visibility Timeout для Redis (период видимости)
Обратите внимание на использование eta (countdown) + visibility_timeout. 
В FAQ описана такая проблема с Redis — так называемый visibility timeout у брокера Redis. 
По умолчанию его значение один час: если через час воркер видит, что задачу никто не взял к исполнению, 
то повторно добавляет ее в очередь. 
Таким образом, если countdown равен двум часам, уже через час брокер выяснит, что эта задача еще не выполнилась, 
и создаст еще одну такую же. А через два часа выполнится две одинаковых задачи.
Если estimation time или countdown превышают 1 час, то, скорее всего, при использовании Redis 
получится дублирование задач, если вы, конечно, 
не изменили значение visibility_timeout в настройках соединения с брокером.
https://docs.celeryproject.org/en/2.2/getting-started/brokers/redis.html
"""

# ==============================================================================================
# ==============================================================================================
"""
ТЕРМИНОЛОГИЯ

Короче говоря, Celery использует набор терминов, которые полезно понимать при построении системы распределенной работы.
•	Клиент - приложение, которое хочет видеть выполненную работу.
•	Worker - приложение, которое выполняет работу
Термины, относящиеся к тем, которые помогают планировать вещи, включают:
•	Брокер - средство, с помощью которого Клиент просит Работника выполнить работу.
•	Приложение - экземпляр класса Celery
На этом этапе обратите внимание, что клиент, брокер и рабочий могут находиться на разных машинах, и на самом деле может быть несколько клиентов на разных машинах и несколько рабочих на разных машинах, если они используют одного и того же брокера.
Поэтому неудивительно, что в приложении обычно настроен брокер с URL-адресом. Это все приложения, все клиенты и рабочие используют один и тот же URL-адрес брокера и, следовательно, все используют одного и того же брокера.
Клиенты отправляют (создают) сообщения через брокера, запрашивая выполнение задач, а Рабочие читают (потребляют) эти сообщения.
Теперь у всех этих терминов есть место:
•	Пул выполнения
•	Кластер
•	Узел
Каждый Worker может обрабатывать несколько задач одновременно, поддерживая пул выполнения. Этот пул может быть потоками или (по умолчанию) подпроцессами. Таким образом, Worker может иметь несколько дочерних процессов Pool.
Одно из разочарований (у меня) с Celery заключается в том, что вы можете свободно общаться с Workers, но не с запущенными задачами в Worker's Execution Pool (по этой причине я создаю новый класс Task для интерактивных задач, но он все еще развивается).
Узел - это просто рабочий в кластере. Короче Node = Worker. Кластер - это ряд рабочих, работающих параллельно ( celery multiв соответствии с документом, с которым я познакомился). Кластер - это просто удобный способ запуска, остановки и управления несколькими рабочими процессами на одной машине.
Однако может быть много кластеров, все выполняющие задачи от одного и того же брокера, и они могут находиться на одной машине (хотя можно задаться вопросом, почему) или на разных машинах.
И это то, что такое узел сельдерея ... (в самом полном контексте).

--------------------------------------------------------------------------

Сам рабочий Celery никаких задач не обрабатывает. 
Он порождает дочерние процессы (или потоки) и занимается всей бухгалтерией. 
Дочерние процессы (или потоки) выполняют фактические задачи. 
Эти дочерние процессы (или потоки) также известны как пул выполнения.
Рабочий (родительский процесс) будет иметь один или несколько рабочих процессов (дочерних процессов).
Таким образом, если какой-либо из дочерних элементов умирает из-за ошибки или из-за максимального ограничения задачи,
родитель может запустить другой дочерний процесс.

--------------------------------------------------------------------------
Отдельное изучение:
- Мультистарт, проблемы с доступом, демонизация
- result_backend –настроить Backend,Управление настройками очистки
- Flower
- Log
- Мониторинг через CLI

"""